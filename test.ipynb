{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-08T15:03:48.922992Z",
     "start_time": "2024-04-08T15:03:48.909061Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"  # Or \"tensorflow\" or \"torch\".\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"0.9\"\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T15:03:56.005505Z",
     "start_time": "2024-04-08T15:03:49.847333Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import keras\n",
    "import keras_nlp"
   ],
   "id": "c67bdb1287ab0cd9",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T15:07:41.673433Z",
     "start_time": "2024-04-08T15:07:41.670385Z"
    }
   },
   "cell_type": "code",
   "source": [
    "os.environ[\"KAGGLE_USERNAME\"] = \"karnwong\"\n",
    "os.environ[\"KAGGLE_KEY\"] = \"4c548a266ca545b1f6f0a25ca4fc65cc\""
   ],
   "id": "7e7620aa09b1df50",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T15:13:21.510775Z",
     "start_time": "2024-04-08T15:07:44.497293Z"
    }
   },
   "cell_type": "code",
   "source": "gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma_2b_en\")\n",
   "id": "2da18c454673df80",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/models/keras/gemma/keras/gemma_2b_en/2/download/config.json...\n",
      "100%|██████████| 555/555 [00:00<00:00, 1.17MB/s]\n",
      "Downloading from https://www.kaggle.com/api/v1/models/keras/gemma/keras/gemma_2b_en/2/download/model.weights.h5...\n",
      "100%|██████████| 4.67G/4.67G [05:09<00:00, 16.2MB/s]  \n",
      "Downloading from https://www.kaggle.com/api/v1/models/keras/gemma/keras/gemma_2b_en/2/download/tokenizer.json...\n",
      "100%|██████████| 401/401 [00:00<00:00, 436kB/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "<class 'keras_nlp.src.models.gemma.gemma_tokenizer.GemmaTokenizer'> could not be deserialized properly. Please ensure that components that are Python object instances (layers, models, etc.) returned by `get_config()` are explicitly deserialized in the model's `from_config()` method.\n\nconfig={'module': 'keras_nlp.src.models.gemma.gemma_tokenizer', 'class_name': 'GemmaTokenizer', 'config': {'name': 'gemma_tokenizer', 'trainable': True, 'dtype': 'int32', 'proto': None, 'sequence_length': None}, 'registered_name': 'keras_nlp>GemmaTokenizer', 'assets': ['assets/tokenizer/vocabulary.spm'], 'weights': None}.\n\nException encountered: Error when deserializing class 'GemmaTokenizer' using config={'name': 'gemma_tokenizer', 'trainable': True, 'dtype': 'int32', 'proto': None, 'sequence_length': None}.\n\nException encountered: GemmaTokenizer requires the `tensorflow-text` package. Please install with `pip install tensorflow-text`.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/article-summarizer-ffxRRXh6-py3.11/lib/python3.11/site-packages/keras/src/ops/operation.py:208\u001B[0m, in \u001B[0;36mOperation.from_config\u001B[0;34m(cls, config)\u001B[0m\n\u001B[1;32m    207\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 208\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    209\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/article-summarizer-ffxRRXh6-py3.11/lib/python3.11/site-packages/keras_nlp/src/models/gemma/gemma_tokenizer.py:83\u001B[0m, in \u001B[0;36mGemmaTokenizer.__init__\u001B[0;34m(self, proto, **kwargs)\u001B[0m\n\u001B[1;32m     81\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpad_token \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m<pad>\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m---> 83\u001B[0m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mproto\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mproto\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/article-summarizer-ffxRRXh6-py3.11/lib/python3.11/site-packages/keras_nlp/src/tokenizers/sentence_piece_tokenizer.py:114\u001B[0m, in \u001B[0;36mSentencePieceTokenizer.__init__\u001B[0;34m(self, proto, sequence_length, dtype, **kwargs)\u001B[0m\n\u001B[1;32m    107\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\n\u001B[1;32m    108\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    109\u001B[0m     proto\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    112\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m    113\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 114\u001B[0m     \u001B[43massert_tf_text_installed\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;18;43m__class__\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;18;43m__name__\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    116\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_int_dtype(dtype) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_string_dtype(dtype):\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/article-summarizer-ffxRRXh6-py3.11/lib/python3.11/site-packages/keras_nlp/src/utils/tensor_utils.py:139\u001B[0m, in \u001B[0;36massert_tf_text_installed\u001B[0;34m(symbol_name)\u001B[0m\n\u001B[1;32m    138\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m tf_text \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 139\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\n\u001B[1;32m    140\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00msymbol_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m requires the `tensorflow-text` package. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    141\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease install with `pip install tensorflow-text`.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    142\u001B[0m     )\n",
      "\u001B[0;31mImportError\u001B[0m: GemmaTokenizer requires the `tensorflow-text` package. Please install with `pip install tensorflow-text`.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/article-summarizer-ffxRRXh6-py3.11/lib/python3.11/site-packages/keras/src/saving/serialization_lib.py:711\u001B[0m, in \u001B[0;36mdeserialize_keras_object\u001B[0;34m(config, custom_objects, safe_mode, **kwargs)\u001B[0m\n\u001B[1;32m    710\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 711\u001B[0m     instance \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_config\u001B[49m\u001B[43m(\u001B[49m\u001B[43minner_config\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    712\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/article-summarizer-ffxRRXh6-py3.11/lib/python3.11/site-packages/keras/src/ops/operation.py:210\u001B[0m, in \u001B[0;36mOperation.from_config\u001B[0;34m(cls, config)\u001B[0m\n\u001B[1;32m    209\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m--> 210\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[1;32m    211\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mError when deserializing class \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m using \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    212\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconfig=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mException encountered: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    213\u001B[0m     )\n",
      "\u001B[0;31mTypeError\u001B[0m: Error when deserializing class 'GemmaTokenizer' using config={'name': 'gemma_tokenizer', 'trainable': True, 'dtype': 'int32', 'proto': None, 'sequence_length': None}.\n\nException encountered: GemmaTokenizer requires the `tensorflow-text` package. Please install with `pip install tensorflow-text`.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m gemma_lm \u001B[38;5;241m=\u001B[39m \u001B[43mkeras_nlp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodels\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mGemmaCausalLM\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_preset\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mgemma_2b_en\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/article-summarizer-ffxRRXh6-py3.11/lib/python3.11/site-packages/keras_nlp/src/models/task.py:254\u001B[0m, in \u001B[0;36mTask.from_preset\u001B[0;34m(cls, preset, load_weights, **kwargs)\u001B[0m\n\u001B[1;32m    252\u001B[0m     preprocessor \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpreprocessor\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    253\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 254\u001B[0m     tokenizer \u001B[38;5;241m=\u001B[39m \u001B[43mload_from_preset\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    255\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpreset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    256\u001B[0m \u001B[43m        \u001B[49m\u001B[43mconfig_file\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtokenizer.json\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    257\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    258\u001B[0m     preprocessor \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39mpreprocessor_cls(tokenizer\u001B[38;5;241m=\u001B[39mtokenizer)\n\u001B[1;32m    259\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mcls\u001B[39m(backbone\u001B[38;5;241m=\u001B[39mbackbone, preprocessor\u001B[38;5;241m=\u001B[39mpreprocessor, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/article-summarizer-ffxRRXh6-py3.11/lib/python3.11/site-packages/keras_nlp/src/utils/preset_utils.py:376\u001B[0m, in \u001B[0;36mload_from_preset\u001B[0;34m(preset, load_weights, config_file, config_overrides)\u001B[0m\n\u001B[1;32m    374\u001B[0m     config \u001B[38;5;241m=\u001B[39m json\u001B[38;5;241m.\u001B[39mload(config_file)\n\u001B[1;32m    375\u001B[0m config[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconfig\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m {\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mconfig[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconfig\u001B[39m\u001B[38;5;124m\"\u001B[39m], \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mconfig_overrides}\n\u001B[0;32m--> 376\u001B[0m layer \u001B[38;5;241m=\u001B[39m \u001B[43mkeras\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msaving\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdeserialize_keras_object\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    378\u001B[0m \u001B[38;5;66;03m# Load any assets for our tokenizers.\u001B[39;00m\n\u001B[1;32m    379\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m get_tokenizer(layer)\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/article-summarizer-ffxRRXh6-py3.11/lib/python3.11/site-packages/keras/src/saving/serialization_lib.py:713\u001B[0m, in \u001B[0;36mdeserialize_keras_object\u001B[0;34m(config, custom_objects, safe_mode, **kwargs)\u001B[0m\n\u001B[1;32m    711\u001B[0m     instance \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39mfrom_config(inner_config)\n\u001B[1;32m    712\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m--> 713\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[1;32m    714\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m could not be deserialized properly. Please\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    715\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m ensure that components that are Python object\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    716\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m instances (layers, models, etc.) returned by\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    717\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m `get_config()` are explicitly deserialized in the\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    718\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m model\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124ms `from_config()` method.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    719\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mconfig=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mException encountered: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    720\u001B[0m     )\n\u001B[1;32m    721\u001B[0m build_config \u001B[38;5;241m=\u001B[39m config\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbuild_config\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m    722\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m build_config \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m instance\u001B[38;5;241m.\u001B[39mbuilt:\n",
      "\u001B[0;31mTypeError\u001B[0m: <class 'keras_nlp.src.models.gemma.gemma_tokenizer.GemmaTokenizer'> could not be deserialized properly. Please ensure that components that are Python object instances (layers, models, etc.) returned by `get_config()` are explicitly deserialized in the model's `from_config()` method.\n\nconfig={'module': 'keras_nlp.src.models.gemma.gemma_tokenizer', 'class_name': 'GemmaTokenizer', 'config': {'name': 'gemma_tokenizer', 'trainable': True, 'dtype': 'int32', 'proto': None, 'sequence_length': None}, 'registered_name': 'keras_nlp>GemmaTokenizer', 'assets': ['assets/tokenizer/vocabulary.spm'], 'weights': None}.\n\nException encountered: Error when deserializing class 'GemmaTokenizer' using config={'name': 'gemma_tokenizer', 'trainable': True, 'dtype': 'int32', 'proto': None, 'sequence_length': None}.\n\nException encountered: GemmaTokenizer requires the `tensorflow-text` package. Please install with `pip install tensorflow-text`."
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "80a41d38e840353"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
